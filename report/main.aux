\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{deng2012mnist}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Dataset generation}{1}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Distribution of labels in a dataset of 80,000 samples.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:manysamples}{{1}{2}{Distribution of labels in a dataset of 80,000 samples}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Distribution of labels in a dataset of 50 samples.}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:fewsamples}{{2}{2}{Distribution of labels in a dataset of 50 samples}{figure.2}{}}
\citation{Zeiler2013relu}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural network pipeline}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Neural networks}{3}{subsection.3.1}\protected@file@percent }
\newlabel{eq:loss}{{1}{3}{Neural networks}{equation.3.1}{}}
\citation{kingma2017adammethodstochasticoptimization}
\citation{kingma2017adammethodstochasticoptimization}
\citation{Mehta_2019}
\citation{Mehta_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Hyperparameter Tuning}{4}{subsection.3.2}\protected@file@percent }
\citation{Mehta_2019}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Values of the hyperparameters of the best-performing neural network.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:nn_hyperparameter}{{1}{5}{Values of the hyperparameters of the best-performing neural network}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The number of neurons in each layer of the best-performing neural network.}}{5}{table.2}\protected@file@percent }
\newlabel{tab:neurons}{{2}{5}{The number of neurons in each layer of the best-performing neural network}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The architecture of the best-performing neural network.}}{6}{table.3}\protected@file@percent }
\newlabel{tab:nn_arch}{{3}{6}{The architecture of the best-performing neural network}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results}{6}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The metrics of the best-performing neural network as calculated by \texttt  {tf.keras. Model.evaluate}.}}{6}{table.4}\protected@file@percent }
\newlabel{tab:nn_eval}{{4}{6}{The metrics of the best-performing neural network as calculated by \texttt {tf.keras. Model.evaluate}}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Confusion matrix of the best-performing neural network. }}{7}{figure.3}\protected@file@percent }
\newlabel{fig:nn_confusion}{{3}{7}{Confusion matrix of the best-performing neural network}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ten images of random digit pairs, their true labels, and the corresponding predictions given by the best-performing neural network.}}{7}{figure.4}\protected@file@percent }
\newlabel{fig:nn_predict}{{4}{7}{Ten images of random digit pairs, their true labels, and the corresponding predictions given by the best-performing neural network}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Other inference algorithms}{8}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}SVM}{8}{subsection.4.1}\protected@file@percent }
\newlabel{eq:fscore}{{10}{8}{SVM}{equation.4.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Values of the hyperparameters of the best-performing SVM.}}{8}{table.5}\protected@file@percent }
\newlabel{tab:svm_hyperparameter}{{5}{8}{Values of the hyperparameters of the best-performing SVM}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Classification report of the best-performing SVM along with the total accuracy of the SVM's performance on the test dataset.}}{9}{table.6}\protected@file@percent }
\newlabel{tab:svm_eval}{{6}{9}{Classification report of the best-performing SVM along with the total accuracy of the SVM's performance on the test dataset}{table.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Confusion matrix of the best-performing SVM.}}{9}{figure.5}\protected@file@percent }
\newlabel{fig:svm_confusion}{{5}{9}{Confusion matrix of the best-performing SVM}{figure.5}{}}
\citation{Zheng2021}
\citation{Breiman2017-zt}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Random forest}{10}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Values of the hyperparameters of the best-performing random forest.}}{10}{table.7}\protected@file@percent }
\newlabel{tab:rf_hyperparameter}{{7}{10}{Values of the hyperparameters of the best-performing random forest}{table.7}{}}
\newlabel{eq:gini}{{11}{10}{Random forest}{equation.4.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Classification report of the best-performing random forest along with the total accuracy of the random forest's performance on the test dataset.}}{11}{table.8}\protected@file@percent }
\newlabel{tab:rf_eval}{{8}{11}{Classification report of the best-performing random forest along with the total accuracy of the random forest's performance on the test dataset}{table.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Confusion matrix of the best-performing random forest.}}{11}{figure.6}\protected@file@percent }
\newlabel{fig:rf_confusion}{{6}{11}{Confusion matrix of the best-performing random forest}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}AdaBoost}{12}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Values of the hyperparameters of the best-performing AdaBoost.}}{12}{table.9}\protected@file@percent }
\newlabel{tab:ab_hyperparameter}{{9}{12}{Values of the hyperparameters of the best-performing AdaBoost}{table.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Confusion matrix of the best-performing AdaBoost classifier.}}{12}{figure.7}\protected@file@percent }
\newlabel{fig:ab_confusion}{{7}{12}{Confusion matrix of the best-performing AdaBoost classifier}{figure.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Classification report of the best-performing AdaBoost along with the total accuracy of the AdaBoost's performance on the test dataset.}}{13}{table.10}\protected@file@percent }
\newlabel{tab:ab_eval}{{10}{13}{Classification report of the best-performing AdaBoost along with the total accuracy of the AdaBoost's performance on the test dataset}{table.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Weak linear classifiers}{13}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Heatmaps of classifier probabilites for a single classifier (left) and a sequentially applied classifier (right).}}{14}{figure.8}\protected@file@percent }
\newlabel{fig:class_prob}{{8}{14}{Heatmaps of classifier probabilites for a single classifier (left) and a sequentially applied classifier (right)}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Accuracy comparison between single and sequential linear regression classifiers as dependant on sample size.}}{14}{figure.9}\protected@file@percent }
\newlabel{fig:class_compare}{{9}{14}{Accuracy comparison between single and sequential linear regression classifiers as dependant on sample size}{figure.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Accuracies of the single and sequential linear regression classifiers as sample sizes increase.}}{15}{table.11}\protected@file@percent }
\newlabel{tab:lin_metric}{{11}{15}{Accuracies of the single and sequential linear regression classifiers as sample sizes increase}{table.11}{}}
\citation{JMLR:v9:vandermaaten08a}
\@writefile{toc}{\contentsline {section}{\numberline {6}t-SNE distributions in neural networks}{16}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Summary}{16}{section.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces t-SNE distribution study as a function of complexity over the raw data and embedding layer of the best-performing neural network.}}{17}{figure.10}\protected@file@percent }
\newlabel{fig:tsne}{{10}{17}{t-SNE distribution study as a function of complexity over the raw data and embedding layer of the best-performing neural network}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Best t-SNE distribution of the input data and embedding layer of the best-performing neural network, optimised over perplexity.}}{18}{figure.11}\protected@file@percent }
\newlabel{fig:best_tsne}{{11}{18}{Best t-SNE distribution of the input data and embedding layer of the best-performing neural network, optimised over perplexity}{figure.11}{}}
\bibstyle{vancouver}
\bibdata{bibliography}
\bibcite{deng2012mnist}{{1}{}{{}}{{}}}
\bibcite{Zeiler2013relu}{{2}{}{{}}{{}}}
\bibcite{kingma2017adammethodstochasticoptimization}{{3}{}{{}}{{}}}
\bibcite{Mehta_2019}{{4}{}{{}}{{}}}
\bibcite{Zheng2021}{{5}{}{{}}{{}}}
\bibcite{Breiman2017-zt}{{6}{}{{}}{{}}}
\bibcite{JMLR:v9:vandermaaten08a}{{7}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {A}Use of auto-generation tools}{19}{appendix.A}\protected@file@percent }
\gdef \@abspage@last{19}
