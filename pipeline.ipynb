{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:15:02.808588: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-23 16:15:03.091742: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732378503.224848   10142 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732378503.257172   10142 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 16:15:03.550109: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1732378511.036526   10142 gpu_device.cc:2022] Created device /device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1732378511.040041   10142 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# importing modules\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input, BatchNormalization\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.test.gpu_device_name()\n",
    "tf.device('/GPU:0')\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 56)\n",
      "y_train shape: (60000,)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 19 #0 - 18\n",
    "np.random.seed(441)\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "def dataset(X, Y):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(len(X)):\n",
    "        i, j = np.random.choice(len(X), size=2, replace=False)\n",
    "        img1, img2 = X[i], X[j]\n",
    "        combined_img = np.hstack((img1, img2))\n",
    "        combined_label = Y[i] + Y[j]\n",
    "        imgs.append(combined_img)\n",
    "        labels.append(combined_label)\n",
    "    return np.array(imgs), np.array(labels)\n",
    "\n",
    "x_train, y_train = dataset(x_train, y_train)\n",
    "x_test, y_test = dataset(x_test, y_test)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     plt.imshow(x_train[i])\n",
    "#     plt.show()\n",
    "#     print(np.where(y_train[i]==1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:16:00,098] Using an existing study with name 'nn_study' instead of creating a new one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c8fadc4afc4171adf0f585a2bb7033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljf1/dis/disvenv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732378565.236400   10317 service.cc:148] XLA service 0x7f7cb401ae80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732378565.236765   10317 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti Laptop GPU, Compute Capability 8.6\n",
      "2024-11-23 16:16:05.301607: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732378565.688378   10317 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-23 16:16:08.440489: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_221', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-23 16:16:08.804968: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_221', 384 bytes spill stores, 384 bytes spill loads\n",
      "\n",
      "2024-11-23 16:16:08.899643: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_221', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-23 16:16:09.047247: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2306', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2024-11-23 16:16:09.059888: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2306', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "I0000 00:00:1732378573.569538   10317 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-11-23 16:16:24.022696: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-23 16:16:24.148904: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_159', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-23 16:16:27.543172: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:18:16,981] Trial 35 finished with value: 0.9039999842643738 and parameters: {'learning_rate': 7.419892844138638e-05, 'n_neurons': 553, 'dropout_rate': 0.00027430010041840823, 'l1_regularization': 8.66456631389732e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:18:34.333051: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2024-11-23 16:18:34.345509: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:20:30,872] Trial 36 finished with value: 0.8920000195503235 and parameters: {'learning_rate': 5.712135940531832e-05, 'n_neurons': 573, 'dropout_rate': 0.0003301071590294027, 'l1_regularization': 9.299140985615297e-05, 'n_layers': 6}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:20:46.316738: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:46.566749: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:46.611836: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 440 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:46.766257: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:46.803226: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:46.838840: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 600 bytes spill stores, 620 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:49.285677: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 252 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:49.286882: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:49.398583: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2024-11-23 16:20:49.706708: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:22:41,448] Trial 37 finished with value: 0.9118000268936157 and parameters: {'learning_rate': 9.001286659072162e-05, 'n_neurons': 592, 'dropout_rate': 0.00020999871133263237, 'l1_regularization': 9.448951520477192e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:22:56.612018: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:22:56.666712: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:24:45,894] Trial 38 finished with value: 0.9089000225067139 and parameters: {'learning_rate': 8.515165465282072e-05, 'n_neurons': 591, 'dropout_rate': 0.00023657076261501245, 'l1_regularization': 9.342414066386545e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:24:59.605748: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-23 16:24:59.747248: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 280 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2024-11-23 16:24:59.958231: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 580 bytes spill stores, 580 bytes spill loads\n",
      "\n",
      "2024-11-23 16:24:59.988511: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 668 bytes spill stores, 680 bytes spill loads\n",
      "\n",
      "2024-11-23 16:25:00.022335: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:25:00.161000: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 176 bytes spill stores, 176 bytes spill loads\n",
      "\n",
      "2024-11-23 16:25:02.822191: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 280 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2024-11-23 16:25:02.939131: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2024-11-23 16:25:03.095828: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:26:55,575] Trial 39 finished with value: 0.9023000001907349 and parameters: {'learning_rate': 7.056006618237664e-05, 'n_neurons': 594, 'dropout_rate': 0.00023116843265550202, 'l1_regularization': 7.257515580657401e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:27:11.580897: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:11.611042: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 436 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:11.750220: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:11.831639: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:11.839838: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 592 bytes spill stores, 612 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:11.989724: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:14.591167: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:14.758158: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:14.904320: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 244 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2024-11-23 16:27:15.028163: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:29:03,277] Trial 40 finished with value: 0.8873000144958496 and parameters: {'learning_rate': 5.1167409388038755e-05, 'n_neurons': 600, 'dropout_rate': 0.00034694056993953806, 'l1_regularization': 9.269061948762207e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:29:17.103966: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 232 bytes spill stores, 232 bytes spill loads\n",
      "\n",
      "2024-11-23 16:29:17.464841: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 512 bytes spill stores, 548 bytes spill loads\n",
      "\n",
      "2024-11-23 16:29:17.548181: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2024-11-23 16:29:20.183619: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 324 bytes spill stores, 424 bytes spill loads\n",
      "\n",
      "2024-11-23 16:29:20.298470: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:31:09,034] Trial 41 finished with value: 0.8899999856948853 and parameters: {'learning_rate': 6.34435755981608e-05, 'n_neurons': 524, 'dropout_rate': 0.0005030993265283683, 'l1_regularization': 8.097894955688012e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:31:23.779623: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:31:23.913117: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:31:26.647620: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 152 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-23 16:31:26.982092: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 152 bytes spill stores, 160 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:33:16,084] Trial 42 finished with value: 0.9049000144004822 and parameters: {'learning_rate': 8.792035762254936e-05, 'n_neurons': 585, 'dropout_rate': 0.0002837115535452584, 'l1_regularization': 2.070355380947577e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:33:30.541237: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:33:30.641793: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:33:33.155294: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 152 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-23 16:33:33.446247: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 152 bytes spill stores, 160 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:35:26,240] Trial 43 finished with value: 0.8999000191688538 and parameters: {'learning_rate': 7.206305391188333e-05, 'n_neurons': 589, 'dropout_rate': 0.00020910967502464544, 'l1_regularization': 9.374668438758258e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:35:40.497081: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2024-11-23 16:35:40.760321: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 392 bytes spill stores, 392 bytes spill loads\n",
      "\n",
      "2024-11-23 16:35:40.860487: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 216 bytes spill stores, 216 bytes spill loads\n",
      "\n",
      "2024-11-23 16:35:40.900718: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:35:40.921716: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 668 bytes spill stores, 680 bytes spill loads\n",
      "\n",
      "2024-11-23 16:35:40.997661: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 284 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2024-11-23 16:35:43.557115: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 284 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2024-11-23 16:35:43.582503: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2024-11-23 16:35:43.758398: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:37:31,277] Trial 44 finished with value: 0.9128999710083008 and parameters: {'learning_rate': 8.799219674880705e-05, 'n_neurons': 582, 'dropout_rate': 0.0001501045383733882, 'l1_regularization': 5.950487116155397e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:37:46.155339: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:37:46.432742: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2024-11-23 16:37:48.979864: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 152 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-23 16:37:48.990074: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 152 bytes spill stores, 160 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:39:37,699] Trial 45 finished with value: 0.90420001745224 and parameters: {'learning_rate': 9.049623835950578e-05, 'n_neurons': 581, 'dropout_rate': 0.00014654976791186435, 'l1_regularization': 6.085825423499135e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:39:52.553090: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2024-11-23 16:39:52.594670: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2024-11-23 16:39:55.220886: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 152 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-23 16:39:55.264921: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 152 bytes spill stores, 160 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:41:44,235] Trial 46 finished with value: 0.8907999992370605 and parameters: {'learning_rate': 6.0502067395865816e-05, 'n_neurons': 593, 'dropout_rate': 0.00024630872645750503, 'l1_regularization': 5.001149182651993e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:41:57.928612: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2024-11-23 16:41:58.071476: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2024-11-23 16:41:58.350448: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 604 bytes spill stores, 624 bytes spill loads\n",
      "\n",
      "2024-11-23 16:41:58.358333: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 444 bytes spill stores, 480 bytes spill loads\n",
      "\n",
      "2024-11-23 16:41:58.392237: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2024-11-23 16:41:58.541427: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "2024-11-23 16:42:00.823108: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2024-11-23 16:42:00.841527: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 256 bytes spill stores, 292 bytes spill loads\n",
      "\n",
      "2024-11-23 16:42:01.004776: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2024-11-23 16:42:01.242165: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_147', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 16:43:50,219] Trial 47 finished with value: 0.9063000082969666 and parameters: {'learning_rate': 7.301249198128112e-05, 'n_neurons': 588, 'dropout_rate': 0.00014921225387433238, 'l1_regularization': 7.429417533380305e-05, 'n_layers': 5}. Best is trial 14 with value: 0.913100004196167.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nn_study.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s_name = \"nn_study\"\n",
    "# m_name = \"nn_model\"\n",
    "\n",
    "# def objective(trial):\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True)\n",
    "#     n_neurons = trial.suggest_int('n_neurons', 500, 600)\n",
    "#     dropout_rate = trial.suggest_float('dropout_rate', 0.0001, 0.001)\n",
    "#     l1_regularization = trial.suggest_float('l1_regularization', 1e-5, 1e-4)\n",
    "#     n_layers = trial.suggest_int('n_layers', 5, 6)\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Flatten(input_shape=(56, 28)))\n",
    "\n",
    "#     for _ in range(n_layers):\n",
    "#         model.add(Dense(n_neurons, activation='relu', kernel_regularizer=l1(l1_regularization)))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "\n",
    "#     model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1, start_from_epoch=10)\n",
    "#     # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "#     model_checkpoint = ModelCheckpoint(f\"best_{m_name}.keras\", save_best_only=True, monitor='val_loss', mode='min', verbose=0)\n",
    "\n",
    "#     model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=15,\n",
    "#                         batch_size=32, verbose=0, callbacks=[model_checkpoint])\n",
    "    \n",
    "#     score = model.evaluate(x_test, y_test, verbose=0)\n",
    "#     accuracy = score[1]\n",
    "\n",
    "#     backend.clear_session()\n",
    "#     del model\n",
    "#     gc.collect()\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', study_name=s_name, storage=f\"sqlite:///{s_name}.db\", load_if_exists=True, sampler=TPESampler(seed=441))\n",
    "\n",
    "# study.optimize(objective, n_trials=13, show_progress_bar=True)\n",
    "# joblib.dump(study, f\"{s_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value: 0.913100004196167\n",
      " Params: \n",
      "    learning_rate: 9.999014745181874e-05\n",
      "    n_neurons: 566\n",
      "    dropout_rate: 0.0002658695997620067\n",
      "    l1_regularization: 4.540153304979403e-05\n",
      "    n_layers: 6\n"
     ]
    }
   ],
   "source": [
    "# print('Best trial:')\n",
    "# print(f' Value: {study.best_trial.value}')\n",
    "# print(' Params: ')\n",
    "# for key, value in study.best_trial.params.items():\n",
    "#     print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m449/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1892 - loss: 5.6829"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:44:00.514681: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2772', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:00.682487: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2772', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.1952 - loss: 5.6519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:44:04.891499: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:04.952995: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 312 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:05.378935: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90_0', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:05.400460: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90_0', 840 bytes spill stores, 840 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:05.512738: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 288 bytes spill stores, 292 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:05.607561: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 668 bytes spill stores, 680 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:05.799014: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:05.986719: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90_0', 1044 bytes spill stores, 1012 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:05.999858: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:06.045170: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 632 bytes spill stores, 636 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:06.132091: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169_0', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:06.159208: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:06.161834: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 992 bytes spill stores, 1008 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:06.246147: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:08.475775: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 288 bytes spill stores, 292 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:08.652805: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-23 16:44:08.689163: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step - accuracy: 0.1955 - loss: 5.6502 - val_accuracy: 0.6033 - val_loss: 3.6782\n",
      "Epoch 2/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6658 - loss: 3.4312 - val_accuracy: 0.7566 - val_loss: 3.0190\n",
      "Epoch 3/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7897 - loss: 2.8981 - val_accuracy: 0.8126 - val_loss: 2.7605\n",
      "Epoch 4/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8437 - loss: 2.6518 - val_accuracy: 0.8322 - val_loss: 2.6165\n",
      "Epoch 5/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8732 - loss: 2.4776 - val_accuracy: 0.8523 - val_loss: 2.4871\n",
      "Epoch 6/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8966 - loss: 2.3360 - val_accuracy: 0.8558 - val_loss: 2.4049\n",
      "Epoch 7/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9116 - loss: 2.2210 - val_accuracy: 0.8674 - val_loss: 2.3151\n",
      "Epoch 8/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9267 - loss: 2.1133 - val_accuracy: 0.8619 - val_loss: 2.2793\n",
      "Epoch 9/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9355 - loss: 2.0247 - val_accuracy: 0.8718 - val_loss: 2.1811\n",
      "Epoch 10/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9470 - loss: 1.9296 - val_accuracy: 0.8625 - val_loss: 2.1661\n",
      "Epoch 11/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9544 - loss: 1.8470 - val_accuracy: 0.8761 - val_loss: 2.0578\n",
      "Epoch 12/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9622 - loss: 1.7663 - val_accuracy: 0.8798 - val_loss: 2.0155\n",
      "Epoch 13/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9673 - loss: 1.6963 - val_accuracy: 0.8827 - val_loss: 1.9392\n",
      "Epoch 14/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 1.6197 - val_accuracy: 0.8872 - val_loss: 1.8755\n",
      "Epoch 15/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9766 - loss: 1.5574 - val_accuracy: 0.8830 - val_loss: 1.8579\n",
      "Epoch 16/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9795 - loss: 1.4956 - val_accuracy: 0.8858 - val_loss: 1.7762\n",
      "Epoch 17/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9831 - loss: 1.4316 - val_accuracy: 0.8881 - val_loss: 1.7348\n",
      "Epoch 18/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9840 - loss: 1.3727 - val_accuracy: 0.8915 - val_loss: 1.6644\n",
      "Epoch 19/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9863 - loss: 1.3178 - val_accuracy: 0.8881 - val_loss: 1.6515\n",
      "Epoch 20/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9823 - loss: 1.2799 - val_accuracy: 0.8906 - val_loss: 1.5761\n",
      "Epoch 21/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9904 - loss: 1.2073 - val_accuracy: 0.8829 - val_loss: 1.5836\n",
      "Epoch 22/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9865 - loss: 1.1727 - val_accuracy: 0.8881 - val_loss: 1.4955\n",
      "Epoch 23/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 1.1137 - val_accuracy: 0.8893 - val_loss: 1.4517\n",
      "Epoch 24/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9890 - loss: 1.0738 - val_accuracy: 0.8941 - val_loss: 1.3952\n",
      "Epoch 25/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 1.0230 - val_accuracy: 0.8915 - val_loss: 1.3765\n",
      "Epoch 26/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.9897 - val_accuracy: 0.8927 - val_loss: 1.3257\n",
      "Epoch 27/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.9487 - val_accuracy: 0.8899 - val_loss: 1.2830\n",
      "Epoch 28/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.9072 - val_accuracy: 0.8898 - val_loss: 1.2579\n",
      "Epoch 29/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.8684 - val_accuracy: 0.8936 - val_loss: 1.2167\n",
      "Epoch 30/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.8483 - val_accuracy: 0.8940 - val_loss: 1.2015\n",
      "Epoch 31/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.7971 - val_accuracy: 0.8945 - val_loss: 1.1530\n",
      "Epoch 32/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9916 - loss: 0.7822 - val_accuracy: 0.8861 - val_loss: 1.1463\n",
      "Epoch 33/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.7501 - val_accuracy: 0.9008 - val_loss: 1.0819\n",
      "Epoch 34/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9956 - loss: 0.7156 - val_accuracy: 0.8963 - val_loss: 1.0651\n",
      "Epoch 35/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 0.7034 - val_accuracy: 0.9010 - val_loss: 1.0287\n",
      "Epoch 36/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.6623 - val_accuracy: 0.8919 - val_loss: 1.0418\n",
      "Epoch 37/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.6587 - val_accuracy: 0.8965 - val_loss: 1.0171\n",
      "Epoch 38/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9965 - loss: 0.6229 - val_accuracy: 0.8905 - val_loss: 1.0178\n",
      "Epoch 39/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9951 - loss: 0.6048 - val_accuracy: 0.9007 - val_loss: 0.9647\n",
      "Epoch 40/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9954 - loss: 0.5865 - val_accuracy: 0.8998 - val_loss: 0.9324\n",
      "Epoch 41/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.5685 - val_accuracy: 0.9006 - val_loss: 0.9186\n",
      "Epoch 42/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.5478 - val_accuracy: 0.8999 - val_loss: 0.9000\n",
      "Epoch 43/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.5420 - val_accuracy: 0.8999 - val_loss: 0.9028\n",
      "Epoch 44/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.5280 - val_accuracy: 0.9091 - val_loss: 0.8569\n",
      "Epoch 45/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.4958 - val_accuracy: 0.9080 - val_loss: 0.8365\n",
      "Epoch 46/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9958 - loss: 0.4880 - val_accuracy: 0.8974 - val_loss: 0.8692\n",
      "Epoch 47/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9923 - loss: 0.4928 - val_accuracy: 0.9027 - val_loss: 0.8457\n",
      "Epoch 48/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9948 - loss: 0.4699 - val_accuracy: 0.9089 - val_loss: 0.8024\n",
      "Epoch 49/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.4434 - val_accuracy: 0.8992 - val_loss: 0.8209\n",
      "Epoch 50/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.4512 - val_accuracy: 0.9002 - val_loss: 0.8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:45:24.137556: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-23 16:45:24.370420: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 288 bytes spill stores, 292 bytes spill loads\n",
      "\n",
      "2024-11-23 16:45:24.679808: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 668 bytes spill stores, 680 bytes spill loads\n",
      "\n",
      "2024-11-23 16:45:24.878755: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_90', 216 bytes spill stores, 216 bytes spill loads\n",
      "\n",
      "2024-11-23 16:45:24.942944: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9002000093460083\n"
     ]
    }
   ],
   "source": [
    "# def create_and_save_model(study):\n",
    "#     with open(\"sampler.pkl\", \"wb\") as fout:\n",
    "#         pickle.dump(study.sampler, fout)\n",
    "        \n",
    "#     params = study.best_trial.params\n",
    "#     model = Sequential()\n",
    "#     model.add(Flatten(input_shape=(56, 28)))\n",
    "#     print()\n",
    "#     for _ in range(params.get('n_layers')):\n",
    "#         model.add(Dense(params.get('n_neurons'), activation='relu', kernel_regularizer=l1(params.get('l1_regularization'))))\n",
    "#         model.add(Dropout(params.get('dropout_rate')))\n",
    "\n",
    "#     model.add( Dense(num_classes,activation='softmax'))\n",
    "\n",
    "#     model.compile(optimizer=Adam(params.get('learning_rate')), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "#     history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50,\n",
    "#                         batch_size=128, verbose=1)\n",
    "\n",
    "#     score = model.evaluate(x_test, y_test, verbose=0)\n",
    "#     accuracy = score[1]\n",
    "#     print(accuracy)\n",
    "#     model.save(f'{m_name}.keras')\n",
    "#     with open('nn_history.pkl', \"wb\") as file:\n",
    "#         pickle.dump(history.history, file)\n",
    "\n",
    "# create_and_save_model(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f'{m_name}.keras')\n",
    "sampler = pickle.load(open(\"sampler.pkl\", \"rb\"))\n",
    "history = pickle.load(open(\"nn_history.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1568</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">888,054</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,922</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,922</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,922</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,922</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,922</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,773</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1568\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │       \u001b[38;5;34m888,054\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │       \u001b[38;5;34m320,922\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │       \u001b[38;5;34m320,922\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │       \u001b[38;5;34m320,922\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │       \u001b[38;5;34m320,922\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │       \u001b[38;5;34m320,922\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m566\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)             │        \u001b[38;5;34m10,773\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,510,313</span> (28.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,510,313\u001b[0m (28.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,437</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,503,437\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,006,876</span> (19.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m5,006,876\u001b[0m (19.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 1568), dtype=float32). Expected shape (None, 56, 28), but input has incompatible shape (32, 1568)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 1568), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mask \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m y_test)\n\u001b[1;32m      2\u001b[0m x_false \u001b[38;5;241m=\u001b[39m x_test[mask]\n",
      "File \u001b[0;32m~/dis/disvenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/dis/disvenv/lib/python3.12/site-packages/keras/src/models/functional.py:264\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 1568), dtype=float32). Expected shape (None, 56, 28), but input has incompatible shape (32, 1568)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 1568), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "mask = (model.predict(np.array(x_test).reshape((10000, 28*56))) != y_test)\n",
    "x_false = x_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)  # Should be (n_samples, n_features)\n",
    "print(y_test.shape)  # Should be (n_samples,) for classification or (n_samples, n_labels) for multi-label tasks\n",
    "predictions = model.predict(x_test)\n",
    "print(predictions.shape)  # Check the shape of the predicted outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(np.array(x_test))[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
